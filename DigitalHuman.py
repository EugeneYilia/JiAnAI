import gradio as gr
import torch
from TTS.api import TTS
from TTS.utils.radam import RAdam
from torch.serialization import add_safe_globals
import whisper
import subprocess
import os
import requests
from opencc import OpenCC

from utils.CutVoice import trim_tail_by_energy_and_gradient

# æ­£ç¡®æ³¨å†Œ RAdam ç±»
add_safe_globals({"RAdam": RAdam})

cc = OpenCC('t2s')

# æ³¨å†Œå…¶ä»–å¯èƒ½éœ€è¦çš„ TTS ç±»ï¼ˆé¿å… torch.load æŠ¥é”™ï¼‰
def safe_register_all_globals():
    torch.serialization._allowed_globals = {
        "__builtin__": set(dir(__builtins__)),
        "builtins": set(dir(__builtins__)),
    }
    torch.serialization._allowed_globals.update({
        "TTS.utils.radam": {"RAdam"},
        "TTS.vocoder.utils.generic": {"ADAM"},
        "TTS.models.tacotron.loss": {"TacotronLoss"},
        "TTS.vocoder.models.wavernn": {"Wavernn"},
    })

safe_register_all_globals()

# åˆå§‹åŒ– TTS æ¨¡å‹
MODEL_NAME = "tts_models/zh-CN/baker/tacotron2-DDC-GST"
tts = TTS(model_name=MODEL_NAME, progress_bar=True, gpu=False)

# åˆå§‹åŒ– Whisper æ¨¡å‹ (ASR)
asr_model = whisper.load_model("base")

# æ¨¡å‹è‡ªåŠ¨ä¸‹è½½å™¨ï¼ˆfor SadTalkerï¼‰
def download_models():
    model_list = [
        ("shape_predictor_68_face_landmarks.dat", "https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2/shape_predictor_68_face_landmarks.dat", "checkpoints"),
        ("wav2lip.pth", "https://huggingface.co/guoyww/facevid2vid/resolve/main/wav2lip.pth", "checkpoints"),
        ("mapping_00109-model.pth.tar", "https://huggingface.co/guoyww/facevid2vid/resolve/main/mapping_00109-model.pth.tar", "checkpoints"),
        ("parsing_model.pth", "https://huggingface.co/guoyww/facevid2vid/resolve/main/parsing_model.pth", "checkpoints"),
        ("GFPGANv1.4.pth", "https://github.com/TencentARC/GFPGAN/releases/download/v1.3.8/GFPGANv1.4.pth", "checkpoints/gfpgan")
    ]

    for name, url, folder in model_list:
        os.makedirs(folder, exist_ok=True)
        dest = os.path.join(folder, name)
        if os.path.exists(dest):
            print(f"[å·²å­˜åœ¨] {name}")
            continue
        print(f"[ä¸‹è½½ä¸­] {name} ...")
        with requests.get(url, stream=True) as r:
            r.raise_for_status()
            with open(dest, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
        print(f"[å®Œæˆ] {dest}")

# åˆæˆè¯­éŸ³
def generate_speech(text):
    output_path = "output.wav"
    tts.tts_to_file(text=text, file_path=output_path)
    trim_tail_by_energy_and_gradient(output_path)
    return output_path

# è¯­éŸ³è½¬æ–‡å­—
def transcribe_audio(audio_file):
    if not audio_file:
        return "âš ï¸ æ²¡æœ‰ä¸Šä¼ ä»»ä½•éŸ³é¢‘æ–‡ä»¶"

    try:
        # ç¡®ä¿ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶å·²å†™å…¥ä¸”ä¸ä¸ºç©º
        if not os.path.exists(audio_file):
            return "â— æ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶ï¼Œè¯·é‡æ–°ä¸Šä¼ "
        if os.path.getsize(audio_file) < 2048:
            return "âš ï¸ éŸ³é¢‘æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½æ— æ•ˆæˆ–ä¸Šä¼ ä¸å®Œæ•´"

        # ä½¿ç”¨ Whisper è¯†åˆ«å¹¶è½¬æ¢ä¸ºç®€ä½“ä¸­æ–‡
        result = asr_model.transcribe(audio_file, language="zh")
        simplified = cc.convert(result["text"])
        return simplified
    except Exception as e:
        return f"è¯†åˆ«å¤±è´¥ï¼š{str(e)}"

# ç”Ÿæˆæ•°å­—äººåŠ¨ç”»ï¼ˆä½¿ç”¨ SadTalker çš„ launcher.pyï¼‰
def generate_video(image_path, audio_path):
    if not image_path or not os.path.exists(image_path):
        return "âš ï¸ æ²¡æœ‰ä¸Šä¼ å¤´åƒå›¾ç‰‡æˆ–æ–‡ä»¶ä¸å­˜åœ¨"
    if os.path.getsize(image_path) < 2048:
        return "âš ï¸ ä¸Šä¼ çš„å¤´åƒæ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½æ— æ•ˆ"

    if not audio_path or not os.path.exists(audio_path):
        return "âš ï¸ æ²¡æœ‰ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶æˆ–æ–‡ä»¶ä¸å­˜åœ¨"
    if os.path.getsize(audio_path) < 2048:
        return "âš ï¸ éŸ³é¢‘æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½æ— æ•ˆæˆ–ä¸Šä¼ ä¸å®Œæ•´"

    output_dir = "results"
    os.makedirs(output_dir, exist_ok=True)

    launcher_path = os.path.abspath("sadtalker/launcher.py")
    cmd = [
        "python", launcher_path,
        "--driven_audio", audio_path,
        "--source_image", image_path,
        "--result_dir", output_dir,
        "--preprocess", "full",
        "--still",
        "--enhancer", "gfpgan"
    ]

    try:
        subprocess.run(cmd, check=True)
    except subprocess.CalledProcessError as e:
        return f"ç”Ÿæˆå¤±è´¥ï¼š\nå‘½ä»¤ï¼š{' '.join(e.cmd)}\nè¿”å›ç ï¼š{e.returncode}"

    output_video_path = os.path.join(output_dir, "result.mp4")
    return output_video_path if os.path.exists(output_video_path) else "ç”Ÿæˆå¤±è´¥ï¼Œæœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶"

# Gradio UI
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ¤ æœ¬åœ°æ•°å­—äººå…¨åŠŸèƒ½å·¥å…·")

    with gr.Tab("æ–‡å­—è½¬è¯­éŸ³"):
        text_input = gr.Textbox(label="è¾“å…¥æ–‡å­—")
        generate_btn = gr.Button("ğŸ§ åˆæˆè¯­éŸ³")
        output_audio = gr.Audio(label="è¯­éŸ³æ–‡ä»¶", type="filepath")
        generate_btn.click(fn=generate_speech, inputs=text_input, outputs=output_audio)

    with gr.Tab("è¯­éŸ³è½¬æ–‡å­—"):
        audio_input = gr.Audio(label="ä¸Šä¼ è¯­éŸ³", type="filepath")
        transcribe_btn = gr.Button("ğŸ“‘ è¯†åˆ«")
        asr_output = gr.Textbox(label="è¯†åˆ«ç»“æœ")
        transcribe_btn.click(fn=transcribe_audio, inputs=audio_input, outputs=asr_output)

    with gr.Tab("æ•°å­—äººåŠ¨ç”»"):
        image_input = gr.Image(label="ä¸Šä¼ å¤´åƒ", type="filepath")
        driven_audio_input = gr.Audio(label="ä½¿ç”¨åˆæˆæˆ–è‡ªå·±è¯­éŸ³", type="filepath")
        generate_video_btn = gr.Button("ğŸ¥ ç”ŸæˆåŠ¨ç”»")
        video_output = gr.Video(label="æ•°å­—äººè§†é¢‘")
        generate_video_btn.click(fn=generate_video, inputs=[image_input, driven_audio_input], outputs=video_output)

    with gr.Tab("æ¨¡å‹ä¸‹è½½"):
        gr.Markdown("### ğŸ§© é¦–æ¬¡ä½¿ç”¨è¯·ç‚¹å‡»ä¸‹è½½ SadTalker æ¨¡å‹")
        download_btn = gr.Button("ğŸ“¥ ä¸‹è½½æ¨¡å‹")
        download_output = gr.Textbox(label="çŠ¶æ€è¾“å‡º")
        download_btn.click(fn=download_models, outputs=download_output)

demo.launch()
