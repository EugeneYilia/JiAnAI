import os
import subprocess
import warnings
import zipfile
from datetime import datetime
import logging

import gradio as gr
import jieba
import requests
import torch
import whisper

from TTS.api import TTS
from TTS.utils.radam import RAdam
from opencc import OpenCC
from torch.serialization import add_safe_globals

from utils.CutVoice import trim_tail_by_energy_and_gradient

warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
jieba.setLogLevel(jieba.logging.WARN)

RECOGNIZED_DIR = "recognized"
os.makedirs(RECOGNIZED_DIR, exist_ok=True)
RECOGNIZED_EXPORT_DIR = "recognized_export"
os.makedirs(RECOGNIZED_EXPORT_DIR, exist_ok=True)

# æ³¨å†Œ RAdam ç±»
add_safe_globals({"RAdam": RAdam})
cc = OpenCC('t2s')

############################################################################
# æ›´æ–°åçš„ CSSï¼š
# 1. å¼ºåˆ¶ html å§‹ç»ˆæ˜¾ç¤ºæ»šåŠ¨æ¡ï¼Œé˜²æ­¢åˆ‡æ¢é¡µç­¾æ—¶å†…å®¹åŒºåŸŸå®½åº¦å˜åŒ–å¯¼è‡´èƒŒæ™¯å›¾ç‰‡æ°´å¹³ç§»åŠ¨ã€‚
# 2. èƒŒæ™¯éƒ¨åˆ†ï¼š
#    - æœ€åº•å±‚ä½¿ç”¨æµ“éƒçš„ç¾Šçš®çº¸è‰²ä½œä¸ºåŸºç¡€ï¼ˆ#f6e2b3ï¼‰ï¼Œ
#    - å åŠ ç»†å¾®çš„é‡å¤çº¿æ€§æ¸å˜ï¼ˆæ¨¡æ‹Ÿçº¸å¼ çº¹ç†ï¼‰ï¼Œ
#    - å†å åŠ åŠé€æ˜ç™½è‰²æ¸å˜ï¼Œ
#    - æœ€ä¸Šå±‚åŠ è½½è¿œç¨‹ freemasonry.pngï¼›å¦‚æœåŠ è½½å¤±è´¥ï¼Œåˆ™åªæ˜¾ç¤ºå‰å‡ å±‚æ•ˆæœã€‚
# 3. ä¸»è¦å†…å®¹å®¹å™¨èƒŒæ™¯è®¾ç½®ä¸º 70% ä¸é€æ˜ã€‚
# 4. å¢åŠ æŒ‰é’®å’Œé¡µç­¾ç‚¹å‡»æ—¶çš„ç¼©æ”¾å“åº”æ•ˆæœã€‚
############################################################################
material_css = """
@import url('https://fonts.googleapis.com/css?family=Roboto:400,500,700&display=swap');

html {
  overflow-y: scroll; /* å§‹ç»ˆæ˜¾ç¤ºæ»šåŠ¨æ¡ */
}

:root {
  --md-primary: #1976d2;
  --md-primary-dark: #1565c0;
  --md-secondary: #424242;
  --md-text: #212121;
  --md-text-on-primary: #ffffff;
  --md-border-radius: 8px;
  --md-transition: 0.3s ease;
}

/* èƒŒæ™¯éƒ¨åˆ†ï¼š
   1. åº•å±‚é‡‡ç”¨æµ“éƒçš„ç¾Šçš®çº¸è‰² (#f6e2b3)
   2. å åŠ ç»†å¾®é‡å¤çº¿æ€§æ¸å˜ï¼ˆ45degï¼Œæ¨¡æ‹Ÿçº¸å¼ çº¹ç†ï¼‰
   3. å†å åŠ åŠé€æ˜ç™½è‰²æ¸å˜
   4. æœ€ä¸Šå±‚åŠ è½½è¿œç¨‹èƒŒæ™¯å›¾ç‰‡ï¼ˆè‹¥åŠ è½½å¤±è´¥ï¼Œåˆ™åªæ˜¾ç¤ºå‰å‡ å±‚æ•ˆæœï¼‰
*/
html, body, .gradio-container {
  margin: 0;
  padding: 0;
  font-family: 'Roboto', sans-serif;
  color: var(--md-text);

  background-color: #f6e2b3 !important; /* ç¾Šçš®çº¸åŸºç¡€è‰² */
  background-image:
    repeating-linear-gradient(45deg, rgba(0,0,0,0.03), rgba(0,0,0,0.03) 1px, transparent 1px, transparent 8px),
    linear-gradient(rgba(255,255,255,0.35), rgba(255,255,255,0.35)),
    url("https://raw.githubusercontent.com/EugeneYilia/JiAnAI/master/assets/images/freemasonry.png");
  background-size: auto, cover, cover;
  background-repeat: repeat, no-repeat, no-repeat;
  background-position: center, center, center;
  background-attachment: fixed, fixed, fixed;
}

/* ä¸»è¦å†…å®¹å®¹å™¨èƒŒæ™¯ï¼š70% ä¸é€æ˜ */
.tabs, .tabitem, .gr-box, .gr-group, .gr-row, .gr-column {
  background-color: rgba(255, 255, 255, 0.7) !important;
  border-radius: var(--md-border-radius) !important;
  box-shadow: 0 2px 8px rgba(0,0,0,0.08);
  margin-top: 8px !important;
  padding: 12px !important;
}

/* è¾“å…¥åŒºåŸŸã€æ–‡ä»¶ä¸Šä¼ ã€éŸ³é¢‘ç»„ä»¶ï¼šçº¯ç™½èƒŒæ™¯ */
.gr-textbox, .gr-file, .gr-audio {
  background-color: #ffffff !important;
  border-radius: var(--md-border-radius) !important;
  box-shadow: 0 1px 3px rgba(0,0,0,0.1);
  padding: 8px !important;
}
.gr-textbox textarea {
  min-height: 100px !important;
  background-color: #fff !important;
  border: 1px solid #ccc !important;
  border-radius: var(--md-border-radius) !important;
}

/* è‡ªå®šä¹‰æŒ‰é’®æ ·å¼ï¼Œä»…é’ˆå¯¹ .gr-button */
.gr-button:hover {
  background-color: var(--md-primary-dark) !important;
  box-shadow: 0 4px 8px rgba(0,0,0,0.2);
}
.gr-button:active {
  transform: scale(0.98);
}

/* Tab æŒ‰é’®é€‰ä¸­çŠ¶æ€åŠç‚¹å‡»å“åº” */
.tabs button.selected {
  color: var(--md-primary) !important;
  border-bottom: 3px solid var(--md-primary) !important;
  background-color: transparent !important;
}
.tabs button:active {
  transform: scale(0.98);
}

/* Footer åŒºåŸŸ */
.footer, .share-link-container {
  text-align: center !important;
  margin-top: 20px;
}
"""

def filter_connection_reset_error(record: logging.LogRecord) -> bool:
    """å¦‚æœæ—¥å¿—æ¶ˆæ¯ä¸­åŒ…å«å¼ºåˆ¶å…³é—­è¿æ¥ç­‰ä¿¡æ¯ï¼Œåˆ™ä¸è¾“å‡º"""
    msg = record.getMessage()
    if "ConnectionResetError" in msg or "forcibly closed by the remote host" in msg:
        return False
    return True

logger_asyncio = logging.getLogger("asyncio")
logger_asyncio.addFilter(filter_connection_reset_error)

############################################################################
# æ³¨å†Œå…¨å±€ & åˆå§‹åŒ– TTS/ASR
############################################################################
def safe_register_all_globals():
    torch.serialization._allowed_globals = {
        "__builtin__": set(dir(__builtins__)),
        "builtins": set(dir(__builtins__)),
    }
    torch.serialization._allowed_globals.update({
        "TTS.utils.radam": {"RAdam"},
        "TTS.vocoder.utils.generic": {"ADAM"},
        "TTS.models.tacotron.loss": {"TacotronLoss"},
        "TTS.vocoder.models.wavernn": {"Wavernn"},
    })

safe_register_all_globals()

MODEL_NAME = "tts_models/zh-CN/baker/tacotron2-DDC-GST"
tts = TTS(model_name=MODEL_NAME, progress_bar=True, gpu=False)
asr_model = whisper.load_model("large")

############################################################################
# è¾…åŠ©å‡½æ•°ï¼šæ ¼å¼åŒ–æ–‡ä»¶å¤§å°
############################################################################
def format_file_size(file_path):
    size_bytes = os.path.getsize(file_path)
    if size_bytes < 1024 * 1024:
        kb = size_bytes / 1024
        return f"{kb:.2f} KB"
    else:
        mb = size_bytes / (1024 * 1024)
        return f"{mb:.2f} MB"

############################################################################
# æ ¸å¿ƒåŠŸèƒ½å‡½æ•°
############################################################################
RECOGNIZED_DIR = "recognized"
RECOGNIZED_EXPORT_DIR = "recognized_export"

os.makedirs(RECOGNIZED_DIR, exist_ok=True)
os.makedirs(RECOGNIZED_EXPORT_DIR, exist_ok=True)

def download_models():
    model_list = [
        ("shape_predictor_68_face_landmarks.dat",
         "https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2/shape_predictor_68_face_landmarks.dat",
         "checkpoints"),
        ("wav2lip.pth", "https://huggingface.co/guoyww/facevid2vid/resolve/main/wav2lip.pth", "checkpoints"),
        ("mapping_00109-model.pth.tar",
         "https://huggingface.co/guoyww/facevid2vid/resolve/main/mapping_00109-model.pth.tar", "checkpoints"),
        ("parsing_model.pth", "https://huggingface.co/guoyww/facevid2vid/resolve/main/parsing_model.pth", "checkpoints"),
        ("GFPGANv1.4.pth", "https://github.com/TencentARC/GFPGAN/releases/download/v1.3.8/GFPGANv1.4.pth",
         "checkpoints/gfpgan")
    ]
    for name, url, folder in model_list:
        os.makedirs(folder, exist_ok=True)
        dest = os.path.join(folder, name)
        if os.path.exists(dest):
            print(f"[å·²å­˜åœ¨] {name}")
            continue
        print(f"[ä¸‹è½½ä¸­] {name} ...")
        with requests.get(url, stream=True) as r:
            r.raise_for_status()
            with open(dest, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
        print(f"[å®Œæˆ] {dest}")

def generate_speech(text):
    output_path = "output.wav"
    tts.tts_to_file(text=text, file_path=output_path)
    trim_tail_by_energy_and_gradient(output_path)
    return output_path

def transcribe_audio(audio_file):
    if not audio_file:
        return "âš ï¸ æ²¡æœ‰ä¸Šä¼ ä»»ä½•éŸ³é¢‘æ–‡ä»¶"
    try:
        if not os.path.exists(audio_file):
            return "â— æ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶ï¼Œè¯·é‡æ–°ä¸Šä¼ "
        if os.path.getsize(audio_file) < 2048:
            return "âš ï¸ éŸ³é¢‘æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½ä¸Šä¼ ä¸å®Œæ•´æˆ–ä¸ºç©ºï¼Œè¯·é‡æ–°ä¸Šä¼ "
        import soundfile as sf
        try:
            _ = sf.info(audio_file)
        except Exception:
            return "âš ï¸ éŸ³é¢‘æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒæˆ–å†…å®¹æŸåï¼Œè¯·é‡æ–°ä¸Šä¼ "
        size_str = format_file_size(audio_file)
        # è¿›è¡Œè¯­éŸ³è¯†åˆ«
        result = asr_model.transcribe(audio_file, language="zh")
        simplified = ""
        for char in result["text"]:
            if char in "ã€‚ï¼ï¼Ÿï¼›ï¼Œã€,.!?;:":
                simplified += char
            else:
                simplified += cc.convert(char)
        save_recognition_history(result["text"], simplified)
        return f"è¯†åˆ«ç»“æœï¼ˆæ–‡ä»¶å¤§å°: {size_str}ï¼‰ï¼š\n{simplified}"
    except Exception as e:
        raise e

def generate_video(image_path, audio_path):
    if not image_path or not os.path.exists(image_path):
        return "âš ï¸ æ²¡æœ‰ä¸Šä¼ å¤´åƒå›¾ç‰‡æˆ–æ–‡ä»¶ä¸å­˜åœ¨"
    if os.path.getsize(image_path) < 2048:
        return "âš ï¸ ä¸Šä¼ çš„å¤´åƒæ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½æ— æ•ˆ"
    if not audio_path or not os.path.exists(audio_path):
        return "âš ï¸ æ²¡æœ‰ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶æˆ–æ–‡ä»¶ä¸å­˜åœ¨"
    if os.path.getsize(audio_path) < 2048:
        return "âš ï¸ éŸ³é¢‘æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½æ— æ•ˆæˆ–ä¸Šä¼ ä¸å®Œæ•´"

    output_dir = "results"
    os.makedirs(output_dir, exist_ok=True)
    launcher_path = os.path.abspath("sadtalker/launcher.py")
    cmd = [
        "python", launcher_path,
        "--driven_audio", audio_path,
        "--source_image", image_path,
        "--result_dir", output_dir,
        "--preprocess", "full",
        "--still",
        "--enhancer", "gfpgan"
    ]
    try:
        subprocess.run(cmd, check=True)
    except subprocess.CalledProcessError as e:
        return f"ç”Ÿæˆå¤±è´¥ï¼š\nå‘½ä»¤ï¼š{' '.join(e.cmd)}\nè¿”å›ç ï¼š{e.returncode}"
    output_video_path = os.path.join(output_dir, "result.mp4")
    return output_video_path if os.path.exists(output_video_path) else "ç”Ÿæˆå¤±è´¥ï¼Œæœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶"

def save_recognition_history(text_raw, text_simplified):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename_txt = os.path.join(RECOGNIZED_DIR, f"recognized_{timestamp}.txt")
    filename_docx = os.path.join(RECOGNIZED_DIR, f"recognized_{timestamp}.docx")

    with open(filename_txt, "w", encoding="utf-8") as f:
        f.write(f"[è¯†åˆ«æ—¶é—´] {timestamp}\n")
        f.write(f"[åŸå§‹æ–‡æœ¬]\n{text_raw}\n\n")
        f.write(f"[ç®€ä½“ç»“æœ]\n{text_simplified}\n")

    from docx import Document
    doc = Document()
    doc.add_heading("è¯­éŸ³è¯†åˆ«ç»“æœ", level=1)
    doc.add_paragraph(f"è¯†åˆ«æ—¶é—´: {timestamp}")
    doc.add_paragraph("åŸå§‹æ–‡æœ¬:")
    doc.add_paragraph(text_raw)
    doc.add_paragraph("è½¬æ¢ä¸ºç®€ä½“ï¼š")
    doc.add_paragraph(text_simplified)
    doc.save(filename_docx)

def export_recognition_zip():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_path = f"recognized_export/recognized_export_{timestamp}.zip"
    with zipfile.ZipFile(zip_path, 'w') as zipf:
        for filename in os.listdir(RECOGNIZED_DIR):
            file_path = os.path.join(RECOGNIZED_DIR, filename)
            zipf.write(file_path, arcname=filename)
    return zip_path

def search_history_by_question(query):
    hits = []
    for filename in os.listdir(RECOGNIZED_DIR):
        path = os.path.join(RECOGNIZED_DIR, filename)
        if filename.endswith(".txt"):
            with open(path, "r", encoding="utf-8") as f:
                content = f.read()
                if query in content:
                    hits.append(f"ğŸ“„ {filename}\n{content[:300]}...\n---")
    if not hits:
        return "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚è¯·å°è¯•è¾“å…¥æ›´å¸¸è§çš„å…³é”®è¯ã€‚"
    return "\n\n".join(hits)

demo = gr.Blocks(css=material_css)

with demo:
    gr.Markdown("""
    <h2 style="text-align:center; color:#0abab5; font-weight:bold; margin-bottom:0.5em;">
    ğŸ¤ å‰å®‰æ™ºèƒ½ä½“
    </h2>
    """)

    with gr.Tab("æ–‡å­—è½¬è¯­éŸ³"):
        text_input = gr.Textbox(label="è¾“å…¥æ–‡å­—")
        generate_btn = gr.Button("ğŸ§ åˆæˆè¯­éŸ³")
        output_audio = gr.Audio(label="è¯­éŸ³æ–‡ä»¶", type="filepath", interactive=True)
        generate_btn.click(fn=generate_speech, inputs=text_input, outputs=output_audio)

    with gr.Tab("è¯­éŸ³è½¬æ–‡å­—"):
        with gr.Row():
            audio_input = gr.File(label="ä¸Šä¼ è¯­éŸ³ (ä»…é™ WAV æ ¼å¼)", file_types=[".wav"], interactive=True)
            upload_status = gr.Textbox(label="è¯­éŸ³ä¸Šä¼ çŠ¶æ€", interactive=False, max_lines=1,
                                       container=True, show_copy_button=True)
        transcribe_btn = gr.Button("ğŸ“‘ è¯†åˆ«")
        asr_output = gr.Textbox(label="è¯†åˆ«ç»“æœ")

        # === ä¿®æ”¹ç‚¹ï¼šå½“ç”¨æˆ·ç§»é™¤æ–‡ä»¶æ—¶ï¼Œä¸å†æ˜¾ç¤ºâ€œæ–‡ä»¶è¿‡å°æˆ–ä¸Šä¼ å¤±è´¥â€ ===
        def check_audio_upload_status(audio_file):
            if not audio_file:
                # ç”¨æˆ·ç§»é™¤äº†æ–‡ä»¶æˆ–ä»æœªä¸Šä¼ ï¼Œä¸æ˜¾ç¤ºä»»ä½•æç¤º
                return ""
            if isinstance(audio_file, str) and os.path.exists(audio_file) and audio_file.endswith('.wav'):
                # å¦‚æœç¡®å®å­˜åœ¨å¹¶ä¸”æ˜¯wavæ–‡ä»¶ï¼Œåˆ™è¿›ä¸€æ­¥æ£€æŸ¥å¤§å°
                if os.path.getsize(audio_file) >= 2048:
                    size_str = format_file_size(audio_file)
                    return f"âœ… éŸ³é¢‘ä¸Šä¼ å®Œæˆ (å¤§å°: {size_str})"
                else:
                    return "âš ï¸ éŸ³é¢‘æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½æ— æ•ˆ"
            # å…¶ä»–æƒ…å†µï¼Œå¦‚éwavæ–‡ä»¶
            return "âš ï¸ è¯·ä¸Šä¼  WAV æ ¼å¼ä¸”å¤§äº2KB çš„éŸ³é¢‘æ–‡ä»¶"

        audio_input.change(fn=check_audio_upload_status, inputs=audio_input, outputs=upload_status)
        transcribe_btn.click(fn=transcribe_audio, inputs=audio_input, outputs=asr_output)

    with gr.Tab("æ•°å­—äººåŠ¨ç”»"):
        with gr.Row():
            with gr.Column():
                image_input = gr.File(label="ä¸Šä¼ å¤´åƒ (PNG/JPG)", file_types=[".png", ".jpg", ".jpeg"],
                                      interactive=True)
                image_name = gr.Textbox(label="å¤´åƒæ–‡ä»¶å", interactive=False, max_lines=1)
                image_status = gr.Textbox(label="å¤´åƒä¸Šä¼ çŠ¶æ€", interactive=False, max_lines=1,
                                          container=True, show_copy_button=True)
                image_preview = gr.Image(label="å¤´åƒé¢„è§ˆ", interactive=False)

                def update_image_preview(image_file):
                    if not image_file or not os.path.exists(image_file):
                        return gr.update(visible=False)
                    if os.path.getsize(image_file) < 2048 or not image_file.lower().endswith((".png", ".jpg", ".jpeg")):
                        return gr.update(visible=False)
                    return gr.update(value=image_file, visible=True)

                image_input.change(fn=update_image_preview, inputs=image_input, outputs=image_preview)

        with gr.Row():
            with gr.Column():
                # å°†æ•°å­—äººåŠ¨ç”»çš„éŸ³é¢‘ä¸Šä¼ ç»„ä»¶æ¢å¤ä¸ºé»˜è®¤ç»„ä»¶ï¼Œé¿å…Content-Lengthé”™è¯¯
                driven_audio_input = gr.Audio(label="ä½¿ç”¨åˆæˆæˆ–è‡ªå·±è¯­éŸ³", type="filepath", interactive=True)
                audio_name = gr.Textbox(label="éŸ³é¢‘æ–‡ä»¶å", interactive=False, max_lines=1)
                audio_status = gr.Textbox(label="éŸ³é¢‘ä¸Šä¼ çŠ¶æ€", interactive=False, max_lines=1,
                                          container=True, show_copy_button=True)

        generate_video_btn = gr.Button("ğŸ¥ ç”ŸæˆåŠ¨ç”»")
        video_output = gr.Video(label="æ•°å­—äººè§†é¢‘")

        # === ä¿®æ”¹ç‚¹ï¼šå½“ç”¨æˆ·ç§»é™¤æ–‡ä»¶æ—¶ï¼Œä¸å†æ˜¾ç¤ºâ€œæ–‡ä»¶è¿‡å°æˆ–ä¸Šä¼ å¤±è´¥â€ ===
        def check_image_upload_status(image_file):
            if not image_file:
                return ""
            if isinstance(image_file, str) and os.path.exists(image_file):
                if os.path.getsize(image_file) >= 2048 and image_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    size_str = format_file_size(image_file)
                    return f"âœ… å¤´åƒä¸Šä¼ å®Œæˆ (å¤§å°: {size_str})"
                else:
                    return "âš ï¸ å¤´åƒæ–‡ä»¶å¤ªå°æˆ–æ ¼å¼ä¸æ­£ç¡®"
            return ""

        def check_audio_upload_status_generic(audio_file):
            if not audio_file:
                return ""
            if isinstance(audio_file, str) and os.path.exists(audio_file):
                if os.path.getsize(audio_file) >= 2048:
                    size_str = format_file_size(audio_file)
                    return f"âœ… éŸ³é¢‘ä¸Šä¼ å®Œæˆ (å¤§å°: {size_str})"
                else:
                    return "âš ï¸ éŸ³é¢‘æ–‡ä»¶å¤ªå°æˆ–æ ¼å¼ä¸æ­£ç¡®"
            return ""

        image_input.change(fn=lambda f: os.path.basename(f) if f else "æœªé€‰æ‹©æ–‡ä»¶", inputs=image_input,
                           outputs=image_name)
        image_input.change(fn=check_image_upload_status, inputs=image_input, outputs=image_status)
        driven_audio_input.change(fn=lambda f: os.path.basename(f) if f else "æœªé€‰æ‹©æ–‡ä»¶", inputs=driven_audio_input,
                                  outputs=audio_name)
        driven_audio_input.change(fn=check_audio_upload_status_generic, inputs=driven_audio_input, outputs=audio_status)

        generate_video_btn.click(fn=generate_video, inputs=[image_input, driven_audio_input], outputs=video_output)

    with gr.Tab("æ¨¡å‹ä¸‹è½½"):
        gr.Markdown("### ğŸ§© é¦–æ¬¡ä½¿ç”¨è¯·ç‚¹å‡»ä¸‹è½½ SadTalker æ¨¡å‹")
        download_btn = gr.Button("ğŸ“¥ ä¸‹è½½æ¨¡å‹")
        download_output = gr.Textbox(label="çŠ¶æ€è¾“å‡º")
        download_btn.click(fn=download_models, outputs=download_output)

    with gr.Tab("è¯†åˆ«å†å²"):
        gr.Markdown("### ğŸ“„ å¯¼å‡ºå†å² / æŸ¥è¯¢å†…å®¹")
        with gr.Row():
            export_btn = gr.Button("ğŸ“¦ å¯¼å‡º ZIP")
            export_file = gr.File(label="ä¸‹è½½è¯†åˆ«è®°å½•å‹ç¼©åŒ…")
            export_btn.click(fn=export_recognition_zip, outputs=export_file)
        with gr.Row():
            query_input = gr.Textbox(label="è¾“å…¥å…³é”®è¯æˆ–å†…å®¹é—®é¢˜")
            query_btn = gr.Button("ğŸ” æŸ¥è¯¢è®°å½•")
            query_result = gr.Textbox(label="æŸ¥è¯¢ç»“æœ", lines=8)
            query_btn.click(fn=search_history_by_question, inputs=query_input, outputs=query_result)

if __name__ == "__main__":
    demo.launch()
