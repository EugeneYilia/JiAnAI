import gradio as gr
import torch
from TTS.api import TTS
from TTS.utils.radam import RAdam
from torch.serialization import add_safe_globals
import whisper
from utils.CutVoice import trim_tail_by_energy_and_gradient
import subprocess
import os

# æ­£ç¡®æ³¨å†Œ RAdam ç±»
add_safe_globals({"RAdam": RAdam})

# å…¼å®¹ TTS æ¨¡å‹ååºåˆ—åŒ–ä¸­å¼•ç”¨çš„å…¶ä»–ç±»
import types

def safe_register_all_globals():
    torch.serialization._allowed_globals = {
        "__builtin__": set(dir(__builtins__)),
        "builtins": set(dir(__builtins__)),
    }
    torch.serialization._allowed_globals.update({
        "TTS.utils.radam": {"RAdam"},
        "TTS.vocoder.utils.generic": {"ADAM"},
        "TTS.models.tacotron.loss": {"TacotronLoss"},
        "TTS.vocoder.models.wavernn": {"Wavernn"},
    })

safe_register_all_globals()

# åˆå§‹åŒ– TTS æ¨¡å‹
MODEL_NAME = "tts_models/zh-CN/baker/tacotron2-DDC-GST"
tts = TTS(model_name=MODEL_NAME, progress_bar=True, gpu=False)

# åˆå§‹åŒ– Whisper æ¨¡å‹ (ASR)
asr_model = whisper.load_model("base")  # å¯æ¢æˆ tiny/small/medium/large

# åˆæˆè¯­éŸ³
def generate_speech(text):
    output_path = "output.wav"
    tts.tts_to_file(text=text, file_path=output_path)
    trim_tail_by_energy_and_gradient(output_path)
    return output_path

# è¯­éŸ³è½¬æ–‡å­—
def transcribe_audio(audio_file):
    if audio_file is None:
        return "è¯·å…ˆä¸Šä¼ è¯­éŸ³æ–‡ä»¶"
    result = asr_model.transcribe(audio_file, language="zh")
    return result["text"]

# ç”Ÿæˆæ•°å­—äººåŠ¨ç”»ï¼ˆä½¿ç”¨ SadTalker çš„ launcher.pyï¼‰
def generate_video(image_path, audio_path):
    if image_path is None or audio_path is None:
        return "è¯·ä¸Šä¼ å›¾ç‰‡å’Œé…éŸ³"
    output_dir = "results"
    os.makedirs(output_dir, exist_ok=True)
    output_video_path = os.path.join(output_dir, "result.mp4")
    cmd = [
        "python", "sadtalker/launcher.py",
        "--driven_audio", audio_path,
        "--source_image", image_path,
        "--result_dir", output_dir,
        "--preprocess", "full",
        "--still",
        "--enhancer", "gfpgan"
    ]
    subprocess.run(cmd, check=True)
    return output_video_path if os.path.exists(output_video_path) else "ç”Ÿæˆå¤±è´¥ï¼Œæœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶"

# Gradio UI
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ¤ æœ¬åœ°æ•°å­—äººå…¨åŠŸèƒ½å·¥å…·")

    with gr.Tab("æ–‡å­—è½¬è¯­éŸ³"):
        text_input = gr.Textbox(label="è¾“å…¥æ–‡å­—")
        generate_btn = gr.Button("ğŸ§ åˆæˆè¯­éŸ³")
        output_audio = gr.Audio(label="è¯­éŸ³æ–‡ä»¶", type="filepath")
        generate_btn.click(fn=generate_speech, inputs=text_input, outputs=output_audio)

    with gr.Tab("è¯­éŸ³è½¬æ–‡å­—"):
        audio_input = gr.Audio(label="ä¸Šä¼ è¯­éŸ³", type="filepath")
        transcribe_btn = gr.Button("ğŸ“‘ è¯†åˆ«")
        asr_output = gr.Textbox(label="è¯†åˆ«ç»“æœ")
        transcribe_btn.click(fn=transcribe_audio, inputs=audio_input, outputs=asr_output)

    with gr.Tab("æ•°å­—äººåŠ¨ç”»"):
        image_input = gr.Image(label="ä¸Šä¼ å¤´åƒ", type="filepath")
        driven_audio_input = gr.Audio(label="ä½¿ç”¨åˆæˆæˆ–è‡ªå·±è¯­éŸ³", type="filepath")
        generate_video_btn = gr.Button("ğŸ¥ ç”ŸæˆåŠ¨ç”»")
        video_output = gr.Video(label="æ•°å­—äººè§†é¢‘")
        generate_video_btn.click(fn=generate_video, inputs=[image_input, driven_audio_input], outputs=video_output)

demo.launch()