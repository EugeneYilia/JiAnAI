import logging
import os
import shutil
import subprocess
import warnings
import zipfile
from datetime import datetime

import gradio as gr
import jieba
import requests
import torch
import whisper
from PIL import Image as PILImage  # ç”¨äºè·å–å›¾åƒåˆ†è¾¨ç‡
from TTS.api import TTS
from opencc import OpenCC

from utils.CutVoice import trim_tail_by_energy_and_gradient

# --------------------------------------------------------------------------
# å¿½ç•¥éƒ¨åˆ†è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
jieba.setLogLevel(jieba.logging.WARN)

# --------------------------------------------------------------------------
# åˆå§‹åŒ–å¿…è¦çš„æ–‡ä»¶å¤¹
UPLOADS_DIR = "uploads"
os.makedirs(UPLOADS_DIR, exist_ok=True)

RECOGNIZED_DIR = "recognized"
os.makedirs(RECOGNIZED_DIR, exist_ok=True)
RECOGNIZED_EXPORT_DIR = "recognized_export"
os.makedirs(RECOGNIZED_EXPORT_DIR, exist_ok=True)

# --------------------------------------------------------------------------
# æ—¥å¿—å™¨è®¾ç½® - åœ¨è„šæœ¬/ä¸»å…¥å£å¤„æ‰§è¡Œ
app_logger = logging.getLogger("app_logger")
app_logger.setLevel(logging.INFO)

while app_logger.handlers:
    app_logger.handlers.pop()

fh = logging.FileHandler(os.path.join(UPLOADS_DIR, "upload.log"), encoding="utf-8")
fh.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(message)s"))
app_logger.addHandler(fh)

app_logger.propagate = False

# æ–°å¢è¯­éŸ³è¯†åˆ«æ—¥å¿—å™¨ï¼Œè®°å½•æ¯ä¸€æ¬¡è¯†åˆ«ç»“æœ
asr_logger = logging.getLogger("asr_logger")
asr_logger.setLevel(logging.INFO)
while asr_logger.handlers:
    asr_logger.handlers.pop()
asr_fh = logging.FileHandler(os.path.join(UPLOADS_DIR, "recognized.log"), encoding="utf-8")
asr_fh.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(message)s"))
asr_logger.addHandler(asr_fh)
asr_logger.propagate = False

def filter_connection_reset_error(record: logging.LogRecord) -> bool:
    msg = record.getMessage()
    if "ConnectionResetError" in msg or "forcibly closed by the remote host" in msg:
        return False
    return True

app_logger.addFilter(filter_connection_reset_error)

asyncio_logger = logging.getLogger("asyncio")
asyncio_logger.propagate = False
asyncio_logger.addFilter(filter_connection_reset_error)

# --------------------------------------------------------------------------
# æ³¨å†Œ RAdam ç±»åŠåˆå§‹åŒ– OpenCC
cc = OpenCC('t2s')

# --------------------------------------------------------------------------
# CSS æ ·å¼è®¾ç½®
material_css = """
@import url('https://fonts.googleapis.com/css?family=Roboto:400,500,700&display=swap');

html {
  overflow-y: scroll; /* å§‹ç»ˆæ˜¾ç¤ºæ»šåŠ¨æ¡ */
}

:root {
  --md-primary: #1976d2;
  --md-primary-dark: #1565c0;
  --md-secondary: #424242;
  --md-text: #212121;
  --md-text-on-primary: #ffffff;
  --md-border-radius: 8px;
  --md-transition: 0.3s ease;
}

/* èƒŒæ™¯éƒ¨åˆ† */
html, body, .gradio-container {
  margin: 0;
  padding: 0;
  font-family: 'Roboto', sans-serif;
  color: var(--md-text);
  background-color: #f6e2b3 !important;
  background-image:
    repeating-linear-gradient(45deg, rgba(0,0,0,0.03), rgba(0,0,0,0.03) 1px, transparent 1px, transparent 8px),
    linear-gradient(rgba(255,255,255,0.35), rgba(255,255,255,0.35)),
    url("https://raw.githubusercontent.com/EugeneYilia/JiAnAI/master/assets/images/freemasonry.png");
  background-size: auto, cover, cover;
  background-repeat: repeat, no-repeat, no-repeat;
  background-position: center, center, center;
  background-attachment: fixed, fixed, fixed;
}

/* ä¸»è¦å†…å®¹å®¹å™¨èƒŒæ™¯ */
.tabs, .tabitem, .gr-box, .gr-group, .gr-row, .gr-column {
  background-color: rgba(255, 255, 255, 0.7) !important;
  border-radius: var(--md-border-radius) !important;
  box-shadow: 0 2px 8px rgba(0,0,0,0.08);
  margin-top: 8px !important;
  padding: 12px !important;
}

/* è¾“å…¥åŒºåŸŸã€æ–‡ä»¶ä¸Šä¼ ã€éŸ³é¢‘ç»„ä»¶ */
.gr-textbox, .gr-file, .gr-audio {
  background-color: #ffffff !important;
  border-radius: var(--md-border-radius) !important;
  box-shadow: 0 1px 3px rgba(0,0,0,0.1);
  padding: 8px !important;
}
.gr-textbox textarea {
  min-height: 100px !important;
  background-color: #fff !important;
  border: 1px solid #ccc !important;
  border-radius: var(--md-border-radius) !important;
}

/* è‡ªå®šä¹‰æŒ‰é’®æ ·å¼ */
.gr-button:hover {
  background-color: var(--md-primary-dark) !important;
  box-shadow: 0 4px 8px rgba(0,0,0,0.2);
}
.gr-button:active {
  transform: scale(0.98);
}

/* Tab æŒ‰é’®é€‰ä¸­çŠ¶æ€åŠç‚¹å‡»å“åº” */
.tabs button.selected {
  color: var(--md-primary) !important;
  border-bottom: 3px solid var(--md-primary) !important;
  background-color: transparent !important;
}
.tabs button:active {
  transform: scale(0.98);
}

/* ğŸš© å¼ºåˆ¶åº•éƒ¨ Footer æ‰€æœ‰å†…å®¹ä¸ºçˆ±é©¬ä»•æ©™ */
footer button.show-api,
footer button.show-api *,
footer button.settings,
footer button.settings *,
footer a.built-with,
footer a.built-with *,
footer .built-with * {
  color: #FF7F00 !important;
  fill: #FF7F00 !important;
  opacity: 1 !important;
  filter: none !important;
  text-shadow: none !important;
}

/* ğŸ¯ ä¸“é—¨å¼ºåˆ¶ä¿®æ”¹ Settings çš„é½¿è½®å›¾æ ‡ */
footer button.settings svg,
footer button.settings path {
  fill: #FF7F00 !important;
  stroke: #FF7F00 !important;
  color: #FF7F00 !important;
}
"""

# --------------------------------------------------------------------------
# å…¨å±€è®¾ç½®åŠåˆå§‹åŒ– TTS/ASR
def safe_register_all_globals():
    torch.serialization._allowed_globals = {
        "__builtin__": set(dir(__builtins__)),
        "builtins": set(dir(__builtins__)),
    }
    torch.serialization._allowed_globals.update({
        "TTS.utils.radam": {"RAdam"},
        "TTS.vocoder.utils.generic": {"ADAM"},
        "TTS.models.tacotron.loss": {"TacotronLoss"},
        "TTS.vocoder.models.wavernn": {"Wavernn"},
    })

safe_register_all_globals()

MODEL_NAME = "tts_models/zh-CN/baker/tacotron2-DDC-GST"
tts = TTS(model_name=MODEL_NAME, progress_bar=True, gpu=False)

# åˆ é™¤åŸæ¥çš„å›ºå®šåŠ è½½ asr æ¨¡å‹ï¼Œæ”¹ä¸ºåŠ¨æ€åŠ è½½
# asr_model = whisper.load_model("large")
# ä½¿ç”¨å­—å…¸ç¼“å­˜ä¸åŒæ¨¡å‹
asr_models = {}

def get_asr_model(model_size):
    """
    æ ¹æ® model_size åŠ è½½ Whisper æ¨¡å‹ï¼Œç¼“å­˜å·²åŠ è½½çš„æ¨¡å‹
    """
    global asr_models
    if model_size not in asr_models:
        asr_models[model_size] = whisper.load_model(model_size)
    return asr_models[model_size]

# --------------------------------------------------------------------------
# è¾…åŠ©å‡½æ•°ï¼šæ ¼å¼åŒ–æ–‡ä»¶å¤§å°
def format_file_size(file_path):
    size_bytes = os.path.getsize(file_path)
    if size_bytes < 1024 * 1024:
        kb = size_bytes / 1024
        return f"{kb:.2f} KB"
    else:
        mb = size_bytes / (1024 * 1024)
        return f"{mb:.2f} MB"

# --------------------------------------------------------------------------
# æ ¸å¿ƒåŠŸèƒ½å‡½æ•°
def download_models():
    model_list = [
        ("shape_predictor_68_face_landmarks.dat",
         "https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2/shape_predictor_68_face_landmarks.dat",
         "checkpoints"),
        ("wav2lip.pth", "https://huggingface.co/guoyww/facevid2vid/resolve/main/wav2lip.pth", "checkpoints"),
        ("mapping_00109-model.pth.tar",
         "https://huggingface.co/guoyww/facevid2vid/resolve/main/mapping_00109-model.pth.tar", "checkpoints"),
        ("parsing_model.pth", "https://huggingface.co/guoyww/facevid2vid/resolve/main/parsing_model.pth", "checkpoints"),
        ("GFPGANv1.4.pth", "https://github.com/TencentARC/GFPGAN/releases/download/v1.3.8/GFPGANv1.4.pth",
         "checkpoints/gfpgan")
    ]
    for name, url, folder in model_list:
        os.makedirs(folder, exist_ok=True)
        dest = os.path.join(folder, name)
        if os.path.exists(dest):
            print(f"[å·²å­˜åœ¨] {name}")
            continue
        print(f"[ä¸‹è½½ä¸­] {name} ...")
        with requests.get(url, stream=True) as r:
            r.raise_for_status()
            with open(dest, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
        print(f"[å®Œæˆ] {dest}")

def move_file_to_uploads(original_path, file_type="unknown"):
    if not original_path or not os.path.exists(original_path):
        return original_path

    abs_uploads = os.path.abspath(UPLOADS_DIR)
    abs_original = os.path.abspath(original_path)
    # å¦‚æœæ–‡ä»¶å·²åœ¨ UPLOADS_DIR å†…ï¼Œåˆ™ç›´æ¥è¿”å›
    if abs_original.startswith(abs_uploads):
        return original_path

    base_name = os.path.basename(original_path)
    new_name = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{file_type}_{base_name}"
    new_path = os.path.join(UPLOADS_DIR, new_name)
    try:
        shutil.copy2(original_path, new_path)
        size_kb = os.path.getsize(new_path) / 1024
        app_logger.info(f"Uploaded {file_type} -> {new_path} ({size_kb:.2f} KB)")
        return new_path
    except Exception as e:
        app_logger.error(f"move_file_to_uploads error: {e}")
        return original_path

def generate_speech(text):
    output_path = "output.wav"
    tts.tts_to_file(text=text, file_path=output_path)
    trim_tail_by_energy_and_gradient(output_path)
    return output_path

def transcribe_audio(audio_file, model_size):
    if not audio_file:
        return "âš ï¸ æ²¡æœ‰ä¸Šä¼ ä»»ä½•éŸ³é¢‘æ–‡ä»¶"
    try:
        if not os.path.exists(audio_file):
            return "â— æ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶ï¼Œè¯·é‡æ–°ä¸Šä¼ "
        if os.path.getsize(audio_file) < 2048:
            return "âš ï¸ éŸ³é¢‘æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½ä¸Šä¼ ä¸å®Œæ•´æˆ–ä¸ºç©ºï¼Œè¯·é‡æ–°ä¸Šä¼ "
        import soundfile as sf
        try:
            _ = sf.info(audio_file)
        except Exception:
            return "âš ï¸ éŸ³é¢‘æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒæˆ–å†…å®¹æŸåï¼Œè¯·é‡æ–°ä¸Šä¼ "

        # å°†æ–‡ä»¶å¤åˆ¶åˆ° /uploads/ï¼ˆè‹¥ä¹‹å‰å·²å¤åˆ¶åˆ™ç›´æ¥ä½¿ç”¨ï¼‰
        new_path = move_file_to_uploads(audio_file, file_type="audio")
        size_str = format_file_size(new_path)

        # æ ¹æ®é€‰å®šçš„æ¨¡å‹å°ºå¯¸åŠ è½½æ¨¡å‹
        model = get_asr_model(model_size)
        result = model.transcribe(new_path, language="zh")

        simplified = ""
        for char in result["text"]:
            if char in "ã€‚ï¼ï¼Ÿï¼›ï¼Œã€,.!?;:":
                simplified += char
            else:
                simplified += cc.convert(char)

        # è®°å½•è¯†åˆ«æ—¥å¿—ï¼Œå†™æ˜ä½¿ç”¨çš„æ¨¡å‹å’Œè¯†åˆ«ç»“æœ
        app_logger.info(f"è¯­éŸ³è¯†åˆ«ä½¿ç”¨æ¨¡å‹: {model_size}ï¼Œè¯†åˆ«ç»“æœ: {simplified}")
        asr_logger.info(f"è¯­éŸ³è¯†åˆ«ä½¿ç”¨æ¨¡å‹: {model_size}ï¼Œè¯†åˆ«ç»“æœ: {simplified}")  # æ–°å¢è®°å½•åˆ° recognized.log
        save_recognition_history(result["text"], simplified, model_size)
        return f"è¯†åˆ«ç»“æœï¼ˆæ–‡ä»¶å¤§å°: {size_str}ï¼‰ï¼š\n{simplified}"
    except Exception as e:
        raise e

def generate_video(image_path, audio_path):
    if not image_path or not os.path.exists(image_path):
        return "âš ï¸ æ²¡æœ‰ä¸Šä¼ å¤´åƒå›¾ç‰‡æˆ–æ–‡ä»¶ä¸å­˜åœ¨"
    if os.path.getsize(image_path) < 2048:
        return "âš ï¸ ä¸Šä¼ çš„å¤´åƒæ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½æ— æ•ˆ"
    if not audio_path or not os.path.exists(audio_path):
        return "âš ï¸ æ²¡æœ‰ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶æˆ–æ–‡ä»¶ä¸å­˜åœ¨"
    if os.path.getsize(audio_path) < 2048:
        return "âš ï¸ éŸ³é¢‘æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½æ— æ•ˆæˆ–ä¸Šä¼ ä¸å®Œæ•´"

    new_audio_path = move_file_to_uploads(audio_path, file_type="audio")

    output_dir = "results"
    os.makedirs(output_dir, exist_ok=True)
    launcher_path = os.path.abspath("sadtalker/launcher.py")
    cmd = [
        "python", launcher_path,
        "--driven_audio", new_audio_path,
        "--source_image", image_path,
        "--result_dir", output_dir,
        "--preprocess", "full",
        "--still",
        "--enhancer", "gfpgan"
    ]
    try:
        subprocess.run(cmd, check=True)
    except subprocess.CalledProcessError as e:
        return f"ç”Ÿæˆå¤±è´¥ï¼š\nå‘½ä»¤ï¼š{' '.join(e.cmd)}\nè¿”å›ç ï¼š{e.returncode}"

    output_video_path = os.path.join(output_dir, "result.mp4")
    return output_video_path if os.path.exists(output_video_path) else "ç”Ÿæˆå¤±è´¥ï¼Œæœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶"

def save_recognition_history(text_raw, text_simplified, model_used):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename_txt = os.path.join(RECOGNIZED_DIR, f"recognized_{timestamp}_{model_used}.txt")
    filename_docx = os.path.join(RECOGNIZED_DIR, f"recognized_{timestamp}_{model_used}.docx")

    with open(filename_txt, "w", encoding="utf-8") as f:
        f.write(f"[è¯†åˆ«æ—¶é—´] {timestamp}\n")
        f.write(f"[ä½¿ç”¨æ¨¡å‹] {model_used}\n")
        f.write(f"[åŸå§‹æ–‡æœ¬]\n{text_raw}\n\n")
        f.write(f"[ç®€ä½“ç»“æœ]\n{text_simplified}\n")

    from docx import Document
    doc = Document()
    doc.add_heading("è¯­éŸ³è¯†åˆ«ç»“æœ", level=1)
    doc.add_paragraph(f"è¯†åˆ«æ—¶é—´: {timestamp}")
    doc.add_paragraph(f"ä½¿ç”¨æ¨¡å‹: {model_used}")
    doc.add_paragraph("åŸå§‹æ–‡æœ¬:")
    doc.add_paragraph(text_raw)
    doc.add_paragraph("è½¬æ¢ä¸ºç®€ä½“ï¼š")
    doc.add_paragraph(text_simplified)
    doc.save(filename_docx)

def export_recognition_zip():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_path = os.path.join(RECOGNIZED_EXPORT_DIR, f"recognized_export_{timestamp}.zip")
    with zipfile.ZipFile(zip_path, 'w') as zipf:
        for filename in os.listdir(RECOGNIZED_DIR):
            file_path = os.path.join(RECOGNIZED_DIR, filename)
            zipf.write(file_path, arcname=filename)
    return zip_path

def search_history_by_question(query):
    hits = []
    for filename in os.listdir(RECOGNIZED_DIR):
        path = os.path.join(RECOGNIZED_DIR, filename)
        if filename.endswith(".txt"):
            with open(path, "r", encoding="utf-8") as f:
                content = f.read()
                if query in content:
                    hits.append(f"ğŸ“„ {filename}\n{content[:300]}...\n---")
    if not hits:
        return "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚è¯·å°è¯•è¾“å…¥æ›´å¸¸è§çš„å…³é”®è¯ã€‚"
    return "\n\n".join(hits)

demo = gr.Blocks(css=material_css)

with demo:
    gr.Markdown("""
    <h2 style="text-align:center; color:#0abab5; font-weight:bold; margin-bottom:0.5em;">
    ğŸ¤ å‰å®‰æ™ºèƒ½ä½“
    </h2>
    """)

    with gr.Tab("æ–‡å­—è½¬è¯­éŸ³"):
        text_input = gr.Textbox(label="è¾“å…¥æ–‡å­—")
        generate_btn = gr.Button("ğŸ§ åˆæˆè¯­éŸ³")
        output_audio = gr.Audio(label="è¯­éŸ³æ–‡ä»¶", type="filepath", interactive=True)
        generate_btn.click(fn=generate_speech, inputs=text_input, outputs=output_audio)

    with gr.Tab("è¯­éŸ³è½¬æ–‡å­—"):
        with gr.Row():
            audio_input = gr.File(label="ä¸Šä¼ è¯­éŸ³ (ä»…é™ WAV æ ¼å¼)", file_types=[".wav"], interactive=True)
            upload_status = gr.Textbox(label="è¯­éŸ³ä¸Šä¼ çŠ¶æ€", interactive=False, max_lines=1,
                                       container=True, show_copy_button=True)
        # ä¸‹æ‹‰èœå•æ”¯æŒæ›´å¤šæ¨¡å‹é€‰é¡¹
        model_selector = gr.Dropdown(choices=["tiny", "base", "small", "medium", "large"],
                                     value="large", label="é€‰æ‹©è¯†åˆ«æ¨¡å‹")
        transcribe_btn = gr.Button("ğŸ“‘ è¯†åˆ«")
        asr_output = gr.Textbox(label="è¯†åˆ«ç»“æœ")

        def check_audio_upload_status(audio_file):
            if not audio_file:
                return ""
            if isinstance(audio_file, str) and os.path.exists(audio_file) and audio_file.endswith('.wav'):
                if os.path.getsize(audio_file) >= 2048:
                    size_str = format_file_size(audio_file)
                    return f"âœ… éŸ³é¢‘ä¸Šä¼ å®Œæˆ (å¤§å°: {size_str})"
                else:
                    return "âš ï¸ éŸ³é¢‘æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½æ— æ•ˆ"
            return "âš ï¸ è¯·ä¸Šä¼  WAV æ ¼å¼ä¸”å¤§äº2KB çš„éŸ³é¢‘æ–‡ä»¶"

        audio_input.change(fn=check_audio_upload_status, inputs=audio_input, outputs=upload_status)
        # è¯†åˆ«æŒ‰é’®åŒæ—¶ä¼ å…¥éŸ³é¢‘æ–‡ä»¶å’Œæ¨¡å‹é€‰æ‹©
        transcribe_btn.click(fn=transcribe_audio, inputs=[audio_input, model_selector], outputs=asr_output)

    with gr.Tab("æ•°å­—äººåŠ¨ç”»"):
        with gr.Row():
            with gr.Column():
                image_input = gr.File(label="ä¸Šä¼ å¤´åƒ (PNG/JPG)", file_types=[".png", ".jpg", ".jpeg"],
                                      interactive=True)
                image_name = gr.Textbox(label="å¤´åƒæ–‡ä»¶å", interactive=False, max_lines=1)
                image_status = gr.Textbox(label="å¤´åƒä¸Šä¼ çŠ¶æ€", interactive=False, max_lines=1,
                                          container=True, show_copy_button=True)
                image_preview = gr.Image(label="å¤´åƒé¢„è§ˆ", interactive=False)

                def update_image_preview(image_file):
                    if not image_file or not os.path.exists(image_file):
                        return gr.update(visible=False, label="")
                    if os.path.getsize(image_file) < 2048 or not image_file.lower().endswith((".png", ".jpg", ".jpeg")):
                        return gr.update(visible=False, label="")
                    new_path = move_file_to_uploads(image_file, file_type="image")
                    im = PILImage.open(new_path)
                    w, h = im.size
                    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    return gr.update(
                        value=new_path,
                        visible=True,
                        label=f"åˆ†è¾¨ç‡: {w}x{h}  |  ä¸Šä¼ æ—¶é—´: {ts}"
                    )

                image_input.change(fn=update_image_preview, inputs=image_input, outputs=image_preview)

        with gr.Row():
            with gr.Column():
                driven_audio_input = gr.Audio(label="ä½¿ç”¨åˆæˆæˆ–è‡ªå·±è¯­éŸ³", type="filepath", interactive=True)
                audio_name = gr.Textbox(label="éŸ³é¢‘æ–‡ä»¶å", interactive=False, max_lines=1)
                audio_status = gr.Textbox(label="éŸ³é¢‘ä¸Šä¼ çŠ¶æ€", interactive=False, max_lines=1,
                                          container=True, show_copy_button=True)

        generate_video_btn = gr.Button("ğŸ¥ ç”ŸæˆåŠ¨ç”»")
        video_output = gr.Video(label="æ•°å­—äººè§†é¢‘")

        def check_image_upload_status(image_file):
            if not image_file:
                return ""
            if isinstance(image_file, str) and os.path.exists(image_file):
                if os.path.getsize(image_file) >= 2048 and image_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    size_str = format_file_size(image_file)
                    return f"âœ… å¤´åƒä¸Šä¼ å®Œæˆ (å¤§å°: {size_str})"
                else:
                    return "âš ï¸ å¤´åƒæ–‡ä»¶å¤ªå°æˆ–æ ¼å¼ä¸æ­£ç¡®"
            return ""

        def check_audio_upload_status_generic(audio_file):
            if not audio_file:
                return ""
            if isinstance(audio_file, str) and os.path.exists(audio_file):
                if os.path.getsize(audio_file) >= 2048:
                    size_str = format_file_size(audio_file)
                    return f"âœ… éŸ³é¢‘ä¸Šä¼ å®Œæˆ (å¤§å°: {size_str})"
                else:
                    return "âš ï¸ éŸ³é¢‘æ–‡ä»¶å¤ªå°æˆ–æ ¼å¼ä¸æ­£ç¡®"
            return ""

        image_input.change(fn=lambda f: os.path.basename(f) if f else "æœªé€‰æ‹©æ–‡ä»¶", inputs=image_input,
                           outputs=image_name)
        image_input.change(fn=check_image_upload_status, inputs=image_input, outputs=image_status)
        driven_audio_input.change(fn=lambda f: os.path.basename(f) if f else "æœªé€‰æ‹©æ–‡ä»¶", inputs=driven_audio_input,
                                  outputs=audio_name)
        driven_audio_input.change(fn=check_audio_upload_status_generic, inputs=driven_audio_input, outputs=audio_status)

        generate_video_btn.click(fn=generate_video, inputs=[image_input, driven_audio_input], outputs=video_output)

    with gr.Tab("æ¨¡å‹ä¸‹è½½"):
        gr.Markdown("### ğŸ§© é¦–æ¬¡ä½¿ç”¨è¯·ç‚¹å‡»ä¸‹è½½ SadTalker æ¨¡å‹")
        download_btn = gr.Button("ğŸ“¥ ä¸‹è½½æ¨¡å‹")
        download_output = gr.Textbox(label="çŠ¶æ€è¾“å‡º")
        download_btn.click(fn=download_models, outputs=download_output)

    with gr.Tab("è¯†åˆ«å†å²"):
        gr.Markdown("### ğŸ“„ å¯¼å‡ºå†å² / æŸ¥è¯¢å†…å®¹")
        with gr.Row():
            export_btn = gr.Button("ğŸ“¦ å¯¼å‡º ZIP")
            export_file = gr.File(label="ä¸‹è½½è¯†åˆ«è®°å½•å‹ç¼©åŒ…")
            export_btn.click(fn=export_recognition_zip, outputs=export_file)
        with gr.Row():
            query_input = gr.Textbox(label="è¾“å…¥å…³é”®è¯æˆ–å†…å®¹é—®é¢˜")
            query_btn = gr.Button("ğŸ” æŸ¥è¯¢è®°å½•")
            query_result = gr.Textbox(label="æŸ¥è¯¢ç»“æœ", lines=8)
            query_btn.click(fn=search_history_by_question, inputs=query_input, outputs=query_result)

if __name__ == "__main__":
    demo.launch()
