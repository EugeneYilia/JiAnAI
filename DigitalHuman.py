import gradio as gr
import torch
from TTS.api import TTS
from TTS.utils.radam import RAdam
from torch.serialization import add_safe_globals
import whisper
import subprocess
import os
import requests
from opencc import OpenCC
import shutil
from fastapi.staticfiles import StaticFiles
import time
import warnings
import jieba
from utils.CutVoice import trim_tail_by_energy_and_gradient

import zipfile
from docx import Document
from datetime import datetime

warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

jieba.setLogLevel(jieba.logging.WARN)

RECOGNIZED_DIR = "recognized"
os.makedirs(RECOGNIZED_DIR, exist_ok=True)

# æ­£ç¡®æ³¨å†Œ RAdam ç±»
add_safe_globals({"RAdam": RAdam})

cc = OpenCC('t2s')

STATIC_AUDIO_DIR = "static/audio"
os.makedirs(STATIC_AUDIO_DIR, exist_ok=True)

demo_config = gr.Blocks()
demo_config.app.mount("/static/audio", StaticFiles(directory=STATIC_AUDIO_DIR), name="audio")

# æ³¨å†Œå…¶ä»–å¯èƒ½éœ€è¦çš„ TTS ç±»ï¼ˆé¿å… torch.load æŠ¥é”™ï¼‰
def safe_register_all_globals():
    torch.serialization._allowed_globals = {
        "__builtin__": set(dir(__builtins__)),
        "builtins": set(dir(__builtins__)),
    }
    torch.serialization._allowed_globals.update({
        "TTS.utils.radam": {"RAdam"},
        "TTS.vocoder.utils.generic": {"ADAM"},
        "TTS.models.tacotron.loss": {"TacotronLoss"},
        "TTS.vocoder.models.wavernn": {"Wavernn"},
    })


safe_register_all_globals()

# åˆå§‹åŒ– TTS æ¨¡å‹
MODEL_NAME = "tts_models/zh-CN/baker/tacotron2-DDC-GST"
tts = TTS(model_name=MODEL_NAME, progress_bar=True, gpu=False)

# åˆå§‹åŒ– Whisper æ¨¡å‹ (ASR)
asr_model = whisper.load_model("large")


# æ¨¡å‹è‡ªåŠ¨ä¸‹è½½å™¨ï¼ˆfor SadTalkerï¼‰
def download_models():
    model_list = [
        ("shape_predictor_68_face_landmarks.dat",
         "https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2/shape_predictor_68_face_landmarks.dat",
         "checkpoints"),
        ("wav2lip.pth", "https://huggingface.co/guoyww/facevid2vid/resolve/main/wav2lip.pth", "checkpoints"),
        ("mapping_00109-model.pth.tar",
         "https://huggingface.co/guoyww/facevid2vid/resolve/main/mapping_00109-model.pth.tar", "checkpoints"),
        (
        "parsing_model.pth", "https://huggingface.co/guoyww/facevid2vid/resolve/main/parsing_model.pth", "checkpoints"),
        ("GFPGANv1.4.pth", "https://github.com/TencentARC/GFPGAN/releases/download/v1.3.8/GFPGANv1.4.pth",
         "checkpoints/gfpgan")
    ]

    for name, url, folder in model_list:
        os.makedirs(folder, exist_ok=True)
        dest = os.path.join(folder, name)
        if os.path.exists(dest):
            print(f"[å·²å­˜åœ¨] {name}")
            continue
        print(f"[ä¸‹è½½ä¸­] {name} ...")
        with requests.get(url, stream=True) as r:
            r.raise_for_status()
            with open(dest, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
        print(f"[å®Œæˆ] {dest}")


# åˆæˆè¯­éŸ³
def generate_speech(text):
    output_path = "output.wav"
    tts.tts_to_file(text=text, file_path=output_path)
    trim_tail_by_energy_and_gradient(output_path)
    return output_path


# è¯­éŸ³è½¬æ–‡å­—
def transcribe_audio(audio_file):
    if not audio_file:
        return "âš ï¸ æ²¡æœ‰ä¸Šä¼ ä»»ä½•éŸ³é¢‘æ–‡ä»¶"

    try:
        # æ–‡ä»¶æ˜¯å¦å­˜åœ¨ + ä¸Šä¼ å®Œæˆæ ¡éªŒ
        if not os.path.exists(audio_file):
            return "â— æ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶ï¼Œè¯·é‡æ–°ä¸Šä¼ "
        if os.path.getsize(audio_file) < 2048:
            return "âš ï¸ éŸ³é¢‘æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½ä¸Šä¼ ä¸å®Œæ•´æˆ–ä¸ºç©ºï¼Œè¯·é‡æ–°ä¸Šä¼ "

        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆéŸ³é¢‘æ–‡ä»¶ï¼ˆå°è¯•ç”¨ ffmpeg è§£ç ï¼‰
        import soundfile as sf
        try:
            _ = sf.info(audio_file)
        except Exception:
            return "âš ï¸ éŸ³é¢‘æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒæˆ–å†…å®¹æŸåï¼Œè¯·é‡æ–°ä¸Šä¼ "

        # Whisper è¯†åˆ«å¹¶è½¬ç®€ä½“ï¼ˆä¿ç•™æ ‡ç‚¹ï¼‰
        result = asr_model.transcribe(audio_file, language="zh")
        simplified = ""
        for char in result["text"]:
            if char in "ã€‚ï¼ï¼Ÿï¼›ï¼Œã€,.!?;:":
                simplified += char
            else:
                simplified += cc.convert(char)
        save_recognition_history(result["text"], simplified)
        return simplified
    except Exception as e:
        return f"è¯†åˆ«å¤±è´¥ï¼š{str(e)}"


# ç”Ÿæˆæ•°å­—äººåŠ¨ç”»ï¼ˆä½¿ç”¨ SadTalker çš„ launcher.pyï¼‰
def generate_video(image_path, audio_path):
    if not image_path or not os.path.exists(image_path):
        return "âš ï¸ æ²¡æœ‰ä¸Šä¼ å¤´åƒå›¾ç‰‡æˆ–æ–‡ä»¶ä¸å­˜åœ¨"
    if os.path.getsize(image_path) < 2048:
        return "âš ï¸ ä¸Šä¼ çš„å¤´åƒæ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½æ— æ•ˆ"

    if not audio_path or not os.path.exists(audio_path):
        return "âš ï¸ æ²¡æœ‰ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶æˆ–æ–‡ä»¶ä¸å­˜åœ¨"
    if os.path.getsize(audio_path) < 2048:
        return "âš ï¸ éŸ³é¢‘æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½æ— æ•ˆæˆ–ä¸Šä¼ ä¸å®Œæ•´"

    output_dir = "results"
    os.makedirs(output_dir, exist_ok=True)

    launcher_path = os.path.abspath("sadtalker/launcher.py")
    cmd = [
        "python", launcher_path,
        "--driven_audio", audio_path,
        "--source_image", image_path,
        "--result_dir", output_dir,
        "--preprocess", "full",
        "--still",
        "--enhancer", "gfpgan"
    ]

    try:
        subprocess.run(cmd, check=True)
    except subprocess.CalledProcessError as e:
        return f"ç”Ÿæˆå¤±è´¥ï¼š\nå‘½ä»¤ï¼š{' '.join(e.cmd)}\nè¿”å›ç ï¼š{e.returncode}"

    output_video_path = os.path.join(output_dir, "result.mp4")
    return output_video_path if os.path.exists(output_video_path) else "ç”Ÿæˆå¤±è´¥ï¼Œæœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶"

def save_recognition_history(text_raw, text_simplified):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename_txt = os.path.join(RECOGNIZED_DIR, f"recognized_{timestamp}.txt")
    filename_docx = os.path.join(RECOGNIZED_DIR, f"recognized_{timestamp}.docx")

    # ä¿å­˜ txt
    with open(filename_txt, "w", encoding="utf-8") as f:
        f.write(f"[è¯†åˆ«æ—¶é—´] {timestamp}\n")
        f.write(f"[åŸå§‹æ–‡æœ¬]\n{text_raw}\n\n")
        f.write(f"[ç®€ä½“ç»“æœ]\n{text_simplified}\n")

    # ä¿å­˜ docx
    doc = Document()
    doc.add_heading("è¯­éŸ³è¯†åˆ«ç»“æœ", level=1)
    doc.add_paragraph(f"è¯†åˆ«æ—¶é—´: {timestamp}")
    doc.add_paragraph("åŸå§‹æ–‡æœ¬ï¼ˆç¹ä½“ï¼‰:")
    doc.add_paragraph(text_raw)
    doc.add_paragraph("è½¬æ¢ä¸ºç®€ä½“ï¼š")
    doc.add_paragraph(text_simplified)
    doc.save(filename_docx)


def export_recognition_zip():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_path = f"recognized_export_{timestamp}.zip"
    with zipfile.ZipFile(zip_path, 'w') as zipf:
        for filename in os.listdir(RECOGNIZED_DIR):
            file_path = os.path.join(RECOGNIZED_DIR, filename)
            zipf.write(file_path, arcname=filename)
    return zip_path

def search_history_by_question(query):
    hits = []
    for filename in os.listdir(RECOGNIZED_DIR):
        path = os.path.join(RECOGNIZED_DIR, filename)
        if filename.endswith(".txt"):
            with open(path, "r", encoding="utf-8") as f:
                content = f.read()
                if query in content:
                    hits.append(f"ğŸ“„ {filename}\n{content[:300]}...\n---")
    if not hits:
        return "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚è¯·å°è¯•è¾“å…¥æ›´å¸¸è§çš„å…³é”®è¯ã€‚"
    return "\n\n".join(hits)

# Gradio UI
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ¤ æœ¬åœ°æ•°å­—äººå…¨åŠŸèƒ½å·¥å…·")

    with gr.Tab("æ–‡å­—è½¬è¯­éŸ³"):
        text_input = gr.Textbox(label="è¾“å…¥æ–‡å­—")
        generate_btn = gr.Button("ğŸ§ åˆæˆè¯­éŸ³")
        output_audio = gr.Audio(label="è¯­éŸ³æ–‡ä»¶", type="filepath", interactive=True)
        generate_btn.click(fn=generate_speech, inputs=text_input, outputs=output_audio)

    with gr.Tab("è¯­éŸ³è½¬æ–‡å­—"):
        with gr.Row():
            audio_input = gr.File(label="ä¸Šä¼ è¯­éŸ³ (ä»…é™ WAV æ ¼å¼)", file_types=[".wav"], interactive=True)
            upload_status = gr.Textbox(label="è¯­éŸ³ä¸Šä¼ çŠ¶æ€", interactive=False, max_lines=1, container=True,
                                       show_copy_button=True)
        transcribe_btn = gr.Button("ğŸ“‘ è¯†åˆ«")
        asr_output = gr.Textbox(label="è¯†åˆ«ç»“æœ")


        def check_audio_upload_status(audio_file):
            if isinstance(audio_file, str) and os.path.exists(audio_file) and os.path.getsize(
                    audio_file) > 2048 and audio_file.endswith('.wav'):
                return "âœ… éŸ³é¢‘ä¸Šä¼ å®Œæˆ"
            return "âš ï¸ éŸ³é¢‘æ–‡ä»¶è¿‡å°æˆ–ä¸Šä¼ å¤±è´¥"


        audio_input.change(fn=check_audio_upload_status, inputs=audio_input, outputs=upload_status)
        transcribe_btn.click(fn=transcribe_audio, inputs=audio_input, outputs=asr_output)

    with gr.Tab("æ•°å­—äººåŠ¨ç”»"):
        with gr.Row():
            with gr.Column():
                image_input = gr.File(label="ä¸Šä¼ å¤´åƒ (PNG/JPG)", file_types=[".png", ".jpg", ".jpeg"],
                                      interactive=True)
                image_name = gr.Textbox(label="å¤´åƒæ–‡ä»¶å", interactive=False, max_lines=1)
                image_status = gr.Textbox(label="å¤´åƒä¸Šä¼ çŠ¶æ€", interactive=False, max_lines=1, container=True,
                                          show_copy_button=True)
                image_preview = gr.Image(label="å¤´åƒé¢„è§ˆ", interactive=False)

                def update_image_preview(image_file):
                    if not image_file or not os.path.exists(image_file):
                        return gr.update(visible=False)
                    if os.path.getsize(image_file) < 2048 or not image_file.lower().endswith((".png", ".jpg", ".jpeg")):
                        return gr.update(visible=False)
                    return gr.update(value=image_file, visible=True)
                image_input.change(fn=update_image_preview, inputs=image_input, outputs=image_preview)

        with gr.Row():
            with gr.Column():
                driven_audio_input = gr.Audio(label="ä½¿ç”¨åˆæˆæˆ–è‡ªå·±è¯­éŸ³", type="filepath", interactive=True,
                                              show_label=True, sources=["upload"], format="wav")
                audio_name = gr.Textbox(label="éŸ³é¢‘æ–‡ä»¶å", interactive=False, max_lines=1)
                audio_status = gr.Textbox(label="éŸ³é¢‘ä¸Šä¼ çŠ¶æ€", interactive=False, max_lines=1, container=True,
                                          show_copy_button=True)

        generate_video_btn = gr.Button("ğŸ¥ ç”ŸæˆåŠ¨ç”»")
        video_output = gr.Video(label="æ•°å­—äººè§†é¢‘")


        def check_image_upload_status(image_file):
            if isinstance(image_file, str) and os.path.exists(image_file) and os.path.getsize(
                    image_file) > 2048 and image_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                return "âœ… å¤´åƒä¸Šä¼ å®Œæˆ"
            return "âš ï¸ å¤´åƒæ–‡ä»¶è¿‡å°æˆ–ä¸Šä¼ å¤±è´¥"


        def check_audio_upload_status_generic(audio_file):
            if audio_file and os.path.exists(audio_file) and os.path.getsize(audio_file) > 2048:
                return "âœ… éŸ³é¢‘ä¸Šä¼ å®Œæˆ"
            return "âš ï¸ éŸ³é¢‘æ–‡ä»¶è¿‡å°æˆ–ä¸Šä¼ å¤±è´¥"


        image_input.change(fn=lambda f: os.path.basename(f) if f else "æœªé€‰æ‹©æ–‡ä»¶", inputs=image_input,
                           outputs=image_name)
        image_input.change(fn=check_image_upload_status, inputs=image_input, outputs=image_status)
        driven_audio_input.change(fn=lambda f: os.path.basename(f) if f else "æœªé€‰æ‹©æ–‡ä»¶", inputs=driven_audio_input,
                                  outputs=audio_name)
        driven_audio_input.change(fn=check_audio_upload_status_generic, inputs=driven_audio_input, outputs=audio_status)

        generate_video_btn.click(fn=generate_video, inputs=[image_input, driven_audio_input], outputs=video_output)

    with gr.Tab("æ¨¡å‹ä¸‹è½½"):
        gr.Markdown("### ğŸ§© é¦–æ¬¡ä½¿ç”¨è¯·ç‚¹å‡»ä¸‹è½½ SadTalker æ¨¡å‹")
        download_btn = gr.Button("ğŸ“¥ ä¸‹è½½æ¨¡å‹")
        download_output = gr.Textbox(label="çŠ¶æ€è¾“å‡º")
        download_btn.click(fn=download_models, outputs=download_output)

    with gr.Tab("è¯†åˆ«å†å²"):
        gr.Markdown("### ğŸ“„ å¯¼å‡ºå†å² / æŸ¥è¯¢å†…å®¹")

        with gr.Row():
            export_btn = gr.Button("ğŸ“¦ å¯¼å‡º ZIP")
            export_file = gr.File(label="ä¸‹è½½è¯†åˆ«è®°å½•å‹ç¼©åŒ…")
            export_btn.click(fn=export_recognition_zip, outputs=export_file)

        with gr.Row():
            query_input = gr.Textbox(label="è¾“å…¥å…³é”®è¯æˆ–å†…å®¹é—®é¢˜")
            query_btn = gr.Button("ğŸ” æŸ¥è¯¢è®°å½•")
            query_result = gr.Textbox(label="æŸ¥è¯¢ç»“æœ", lines=8)
            query_btn.click(fn=search_history_by_question, inputs=query_input, outputs=query_result)

demo.launch(share=True)
